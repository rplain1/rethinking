---
title: "chapter_6_brms"
format: html
---


```{r}
library(tidyverse)
library(brms)
library(bayesplot)
library(tidybayes)
library(ggdag)
Sys.setenv(RSTUDIO = 1)
```

## Multicollinearity

### Legs

```{r}
n <- 100
set.seed(909)

d <-
  tibble(
    height = rnorm(n, mean = 10, sd = 2),
    leg_prop = runif(n, min = 0.4, max = 0.5)
  ) %>%
    mutate(
    leg_left = leg_prop * height + rnorm(n, mean = 0, sd = 0.02),
    leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02)
  )
```



```{r}
b6.1 <-
  brm(
    data = d,
    family = gaussian,
    height ~ 1 + leg_left + leg_right,
    prior = c(
      prior(normal(10, 100), class = Intercept),
      prior(normal(2, 10), class = b),
      prior(exponential(1), class = sigma)
    ),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    cores = 4,
    seed = 6,
    file = "fits/b06.01"
  )
```



```{r}
print(b6.1)


mcmc_plot(
  b6.1,
  type = "intervals",
  prob = .5,
  prob_outer = .95,
  point_est = "mean"
) +
  labs(
    title = "The coefficient plot for the two-leg model",
    subtitle = "Holy smokes; look at the widths of those betas!"
  ) +
  theme(
    axis.text.y = element_text(hjust = 0),
    panel.grid.minor = element_blank(),
    strip.text = element_text(hjust = 0)
  )
```

Add the coefficients together, and you get close to 2, which is what we would expect given that height is 2x the length of legs typically.
```{r}
pairs(b6.1, variable = variables(b6.1)[2:3])

post <- as_draws_df(b6.1)

post %>%
  ggplot(aes(x = b_leg_left, y = b_leg_right)) +
  geom_point(color = "forestgreen", alpha = 1 / 10, size = 1 / 2)

post %>%
  ggplot(aes(x = b_leg_left + b_leg_right, y = 0)) +
  tidybayes::stat_halfeye(point_interval = median_qi, fill = "steelblue", .width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(
    title = "Sum the multicollinear coefficients",
    subtitle = "Marked by the median and 95% PIs"
  )
```

#### Fit the revised model


```{r}
b6.2 <-
  brm(
    data = d,
    family = gaussian,
    height ~ 1 + leg_left,
    prior = c(
      prior(normal(10, 100), class = Intercept),
      prior(normal(2, 10), class = b),
      prior(exponential(1), class = sigma)
    ),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    cores = 4,
    seed = 6,
    file = "fits/b06.02"
  )
```


```{r}
print(b6.2)
```

### Multicollinear Milk


```{r}
data(milk, package = "rethinking")
d <- milk
rm(milk)

d <-
  d %>%
  mutate(
    k = rethinking::standardize(kcal.per.g),
    f = rethinking::standardize(perc.fat),
    l = rethinking::standardize(perc.lactose)
  )
```


```{r}
# k regressed on f
b6.3 <-
  brm(
    data = d,
    family = gaussian,
    k ~ 1 + f,
    prior = c(
      prior(normal(0, 0.2), class = Intercept),
      prior(normal(0, 0.5), class = b),
      prior(exponential(1), class = sigma)
    ),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    cores = 4,
    seed = 6,
    file = "fits/b06.03"
  )

# k regressed on l
b6.4 <-
  update(b6.3, newdata = d, formula = k ~ 1 + l, seed = 6, file = "fits/b06.04")
```


```{r}
posterior_summary(b6.3)[1:3, ] %>% round(digits = 2)
posterior_summary(b6.4)[1:3, ] %>% round(digits = 2)

```

Adding two correlated predictors into the model


```{r}
b6.5 <-
  update(
    b6.4,
    newdata = d,
    formula = k ~ 1 + f + l,
    seed = 6,
    file = "fits/b06.05"
  )

posterior_summary(b6.5)[1:3, ] %>% round(digits = 2)

```

```{r}
d %>%
  select(kcal.per.g, perc.fat, perc.lactose) %>%
  pairs(col = "forestgreen")
```

## Post-treatment bias


```{r}
# how many plants would you like?
n <- 100

set.seed(71)
d <-
  tibble(
    h0 = rnorm(n, mean = 10, sd = 2),
    treatment = rep(0:1, each = n / 2),
    fungus = rbinom(n, size = 1, prob = .5 - treatment * 0.4),
    h1 = h0 + rnorm(n, mean = 5 - 3 * fungus, sd = 1)
  )

d %>%
  pivot_longer(everything()) %>%
  group_by(name) %>%
  mean_qi(.width = .89) %>%
  mutate_if(is.double, round, digits = 2)
```

A Prior is born

LogNormal(0, 0.25)

We are using value of `h1` / `h2` as what we are predicting, `p`.

```{r}
set.seed(6)

# simulate
sim_p <-
  tibble(sim_p = rlnorm(1e4, meanlog = 0, sdlog = 0.25))

# wrangle
sim_p %>%
  mutate(`exp(sim_p)` = exp(sim_p)) %>%
  gather() %>%

  # plot
  ggplot(aes(x = value)) +
  geom_density(fill = "steelblue") +
  scale_x_continuous(breaks = c(0, .5, 1, 1.5, 2, 3, 5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 6)) +
  theme(panel.grid.minor.x = element_blank()) +
  facet_wrap(~key, scale = "free_y", ncol = 1)


```

Fit the new height with only the original height as the predictor and without an intercept. This uses the lognormal prior to estimate the growth

The prior in McElreath's model expects anything form a 40% shrinkage to 60% growth. Which is similar to what `hist(rlnorm(0, 0.25))` shows.


```{r}
b6.6 <-
  brm(
    data = d,
    family = gaussian,
    h1 ~ 0 + h0,
    prior = c(
      prior(lognormal(0, 0.25), class = b, lb = 0),
      prior(exponential(1), class = sigma)
    ),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    cores = 4,
    seed = 6,
    file = "fits/b06.06"
  )
```


```{r}
print(b6.6)
broom.mixed::tidyMCMC(b6.6)

```

The prior model does okay _modeling_ the scenario, but without any predictors. Lets update the model

We are going to add in `treatment` and `fungus` as predictors.

Solomon takes the following:

$$
\begin{align}
\mu_i = h_{0i} \times p \\
p = \alpha + \beta_1treatment_i + \beta_2fungus_i
\end{align}
$$

and reframes it with algebra to this:

$$
\begin{align}
\mu_i = h_{0i} \times ( \alpha + \beta_1treatment_i + \beta_2fungus_i)
\end{align}
$$

##### Non-linear model in brms

```{r}
b6.7 <-
  brm(
    data = d,
    family = gaussian,
    bf(h1 ~ h0 * (a + t * treatment + f * fungus), a + t + f ~ 1, nl = TRUE),
    prior = c(
      prior(lognormal(0, 0.2), nlpar = a, lb = 0),
      prior(normal(0, 0.5), nlpar = t),
      prior(normal(0, 0.5), nlpar = f),
      prior(exponential(1), class = sigma)
    ),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    cores = 4,
    seed = 6,
    file = "fits/b06.07"
  )
```


```{r}
print(b6.7)
broom.mixed::tidyMCMC(b6.7)
```

##### Omit Fungus from model

In the above model, we matched McElreath's results, but both models do not fit the data generated.

The treatment effect is 0, and the fungus effect is negative.

```{r}
b6.8 <-
  brm(
    data = d,
    family = gaussian,
    bf(h1 ~ h0 * (a + t * treatment), a + t ~ 1, nl = TRUE),
    prior = c(
      prior(lognormal(0, 0.2), nlpar = a, lb = 0),
      prior(normal(0, 0.5), nlpar = t),
      prior(exponential(1), class = sigma)
    ),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    cores = 4,
    seed = 6,
    file = "fits/b06.08"
  )
```


```{r}
print(b6.8)
broom.mixed::tidyMCMC(b6.8)
```

Now the model shows a postive effect for treatment. The model for `b6.7` was answering:
_once we already know whether or not a plant developed fungus, does soil treatment matter?_

The answer was "no", because soil treatment has its effects on growth through _reducing_ fungus.

It can not eliminate it.

### Fungus and _d_-separation


```{r}
# define our coordinates
dag_coords <-
  tibble(name = c("H0", "T", "F", "H1"), x = c(1, 5, 4, 3), y = c(2, 2, 1.5, 1))

# save our DAG
dag <-
  dagify(F ~ T, H1 ~ H0 + F, coords = dag_coords)

# plot
dag %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(color = "steelblue", alpha = 1 / 2, size = 6.5) +
  geom_dag_text(color = "black") +
  geom_dag_edges() +
  theme_dag()
```

Function to recreate dags
```{r}
gg_simple_dag <- function(d) {
  d %>%
    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
    geom_dag_point(color = "steelblue", alpha = 1 / 2, size = 6.5) +
    geom_dag_text(color = "black") +
    geom_dag_edges() +
    theme_dag()
}

# try it out!
dag %>%
  gg_simple_dag()
```

Conditioning on `fungus` introduces `d-separation`, where _d_ stands for _directional_.

In this case, conditioning on `fungus` effectively  blocks the directed path of
T --> F --> H_1, making T and H_1 independent (d-separated)

In other words, once we know F, T provides little information.

##### DAG of a different kind of structure

Introducing M, an unobserved variable between H1 and F.

In this case, T influences F, but F does not influence H1. If we include T in a model,
it will show no effect. However if we include F, it will show and effect because unobserved
M is shared between F and H1.

```{r}
# define our coordinates
dag_coords <-
  tibble(
    name = c("H0", "H1", "M", "F", "T"),
    x = c(1, 2, 2.5, 3, 4),
    y = c(2, 2, 1, 2, 2)
  )

# save our DAG
dag <-
  dagify(F ~ M + T, H1 ~ H0 + M, coords = dag_coords)

# plot
dag %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(
    aes(color = name == "M"),
    alpha = 1 / 2,
    size = 6.5,
    show.legend = F
  ) +
  geom_point(
    x = 2.5,
    y = 1,
    size = 6.5,
    shape = 1,
    stroke = 1,
    color = "orange"
  ) +
  geom_dag_text(color = "black") +
  geom_dag_edges() +
  scale_color_manual(values = c("steelblue", "orange")) +
  theme_dag()
```


```{r}
gg_fancy_dag <- function(d, x = 1, y = 1, circle = "U") {
  d %>%
    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
    geom_dag_point(
      aes(color = name == circle),
      alpha = 1 / 2,
      size = 6.5,
      show.legend = F
    ) +
    geom_point(
      x = x,
      y = y,
      size = 6.5,
      shape = 1,
      stroke = 1,
      color = "orange"
    ) +
    geom_dag_text(color = "black") +
    geom_dag_edges() +
    scale_color_manual(values = c("steelblue", "orange")) +
    theme_dag()
}

# check it out
dag %>%
  gg_fancy_dag(x = 2.5, y = 1, circle = "M")
```


Simulating new data with M

```{r}
set.seed(71)
n <- 1000

d2 <-
  tibble(
    h0 = rnorm(n, mean = 10, sd = 2),
    treatment = rep(0:1, each = n / 2),
    m = rbinom(n, size = 1, prob = .5),
    fungus = rbinom(n, size = 1, prob = .5 - treatment * 0.4 + 0.4 * m),
    h1 = h0 + rnorm(n, mean = 5 + 3 * m, sd = 1)
  )

head(d2)
```


```{r}
b6.7b <-
  update(b6.7, newdata = d2, seed = 6, file = "fits/b06.07b")

b6.8b <-
  update(b6.8, newdata = d2, seed = 6, file = "fits/b06.08b")
```

This takes the two modesl, one with fungus and one without, and uses the data simulated
with the unobserved M variable. (Moisture).

The model with fungus shows a positive effect with fungus and treatment
The model without fungus shows no effect with treatment

“Including fungus again confounds inference about the treatment,
this time by making it seem like it helped the plants,
even though it had no effect” (p. 175).

```{r}
posterior_summary(b6.7b)[1:4, ] %>% round(digits = 2)
posterior_summary(b6.8b)[1:3, ] %>% round(digits = 2)
```


## Collider Bias

Back to the trustworthiness/newsworthiness example

T --> S <-- N

When you condition on a collider, it makes statisticcal - but not necessarily causal --
associations.

```{r}
dag_coords <-
  tibble(name = c("T", "S", "N"), x = 1:3, y = 1)

dagify(S ~ T + N, coords = dag_coords) %>%
  gg_simple_dag()
```

In this case, once you lear that a proposal has been selected (S), then learning
trustworthiness (T) also provides information about newsworthiness (N).

#### Sim Happiness


```{r}
new_borns <- function(n = 20) {
  tibble(
    a = 1, # 1 year old
    m = 0, # not married
    h = seq(from = -2, to = 2, length.out = n)
  ) # range of happiness scores
}

new_borns()

update_population <- function(pop, n_births = 20, aom = 18, max_age = 65) {
  pop %>%
    mutate(
      a = a + 1, # everyone gets one year older
      # some people get married
      m = ifelse(
        m >= 1,
        1,
        (a >= aom) * rbinom(n(), 1, rethinking::inv_logit(h - 4))
      )
    ) %>%
    filter(a <= max_age) %>% # old people die
    bind_rows(new_borns(n_births)) # new people are born
}

new_borns() |>
  update_population()
```


```{r}
# this was McElreath's seed
set.seed(1977)

# year 1
d <- new_borns(n = 20)

# years 2 through 1000
for (i in 2:1000) {
  d <- update_population(d, n_births = 20, aom = 18, max_age = 65)
}

# now rename()
d <-
  d %>%
  rename(age = a, married = m, happiness = h)

# take a look
glimpse(d)
```


```{r}
d

d %>%
  pivot_longer(everything()) %>%
  group_by(name) %>%
  mean_qi(value) %>%
  mutate_if(is.double, round, digits = 2)
```

People that are older or happier are married. The correlation between age and happiness looks negative if you
condition on marriage.

```{r}
d %>%
  mutate(married = factor(married, labels = c("unmarried", "married"))) %>%

  ggplot(aes(x = age, y = happiness, color = married)) +
  geom_point(size = 1.75) +
  scale_color_manual(NULL, values = c("grey85", "forestgreen")) +
  scale_x_continuous(expand = c(.015, .015)) +
  theme(panel.grid = element_blank())
```

# filter out older samples that are able to be married

```{r}
d2 <-
  d %>%
  filter(age > 17) %>%
  mutate(a = (age - 18) / (65 - 18))

head(d2)
```


```{r}
d2 <-
  d2 %>%
  mutate(mid = factor(married + 1, labels = c("single", "married")))

head(d2)
```

#### Fitting the model on d2


```{r}
b6.9 <-
  brm(
    data = d2,
    family = gaussian,
    happiness ~ 0 + mid + a,
    prior = c(
      prior(normal(0, 1), class = b, coef = midmarried),
      prior(normal(0, 1), class = b, coef = midsingle),
      prior(normal(0, 2), class = b, coef = a),
      prior(exponential(1), class = sigma)
    ),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    cores = 4,
    seed = 6,
    file = "fits/b06.09"
  )
```

```{r}
print(b6.9)
```

Now drop `mid`

```{r}
b6.10 <-
  brm(
    data = d2,
    family = gaussian,
    happiness ~ 0 + Intercept + a,
    prior = c(
      prior(normal(0, 1), class = b, coef = Intercept),
      prior(normal(0, 2), class = b, coef = a),
      prior(exponential(1), class = sigma)
    ),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    cores = 4,
    seed = 6,
    file = "fits/b06.10"
  )
```


```{r}
print(b6.10)
```

It looks like in the first model that age is negatively associated with happiness. This is just a
statistical asociation, not a causal association. Once we know whether someone is married or not,
then their age does provide information about how happy they are.

```{r}
posterior_summary(b6.9)[1:4, ] %>% round(digits = 2)
posterior_summary(b6.10)[1:3, ] %>% round(digits = 2)
```

### The haunted DAG


```{r}
dag_coords <-
  tibble(name = c("G", "P", "C"), x = c(1, 2, 2), y = c(2, 2, 1))

dagify(P ~ G, C ~ P + G, coords = dag_coords) %>%
  gg_simple_dag()
```


```{r}
dag_coords <-
  tibble(name = c("G", "P", "C", "U"), x = c(1, 2, 2, 2.5), y = c(2, 2, 1, 1.5))

dagify(P ~ G + U, C ~ P + G + U, coords = dag_coords) %>%
  gg_fancy_dag(x = 2.5, y = 1.5, circle = "U")
```

Simulate groups of grandparents, parents, and children
Include unknown variable with effect on on P and C

```{r}
# how many grandparent-parent-child triads would you like?
n <- 200

b_gp <- 1 # direct effect of G on P
b_gc <- 0 # direct effect of G on C
b_pc <- 1 # direct effect of P on C
b_u <- 2 # direct effect of U on P and C

# simulate triads
set.seed(1)
d <-
  tibble(
    u = 2 * rbinom(n, size = 1, prob = .5) - 1,
    g = rnorm(n, mean = 0, sd = 1)
  ) %>%
  mutate(p = rnorm(n, mean = b_gp * g + b_u * u, sd = 1)) %>% # g --> p <-- u
  mutate(c = rnorm(n, mean = b_pc * p + b_gc * g + b_u * u, sd = 1)) # g --> p --> c < -- u

head(d)
```

#### Fit the model without `u`

```{r}
b6.11 <-
  brm(
    data = d,
    family = gaussian,
    c ~ 0 + Intercept + p + g,
    prior = c(
      prior(normal(0, 1), class = b),
      prior(exponential(1), class = sigma)
    ),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    cores = 4,
    seed = 6,
    file = "fits/b06.11"
  )
```


```{r}
print(b6.11)
broom.mixed::tidyMCMC(b6.11)
```

Unobserved confounds in collider

We have bad neighborhoods and good neighborhoods, `u`, that are not in the model. This unobserved
variable can make it look like grandparents have a negative effect on children.
```{r}
d %>%
  mutate(
    centile = ifelse(
      p >= quantile(p, prob = .45) & p <= quantile(p, prob = .60),
      "a",
      "b"
    ),
    u = factor(u)
  ) %>%

  ggplot(aes(x = g, y = c)) +
  geom_point(aes(shape = centile, color = u), size = 2.5, stroke = 1 / 4) +
  stat_smooth(
    data = . %>% filter(centile == "a"),
    method = "lm",
    se = F,
    linewidth = 1 / 2,
    color = "black",
    fullrange = T
  ) +
  scale_shape_manual(values = c(19, 1)) +
  scale_color_manual(values = c("black", "lightblue")) +
  theme(legend.position = "none")
```

#### Fit including `u`

```{r}
b6.12 <-
  update(
    b6.11,
    newdata = d,
    formula = c ~ 0 + Intercept + p + g + u,
    seed = 6,
    file = "fits/b06.12"
  )
```

Now the coefficient of `b_g` is around 0, which is the effect of the simulated data we expected.
```{r}
print(b6.12)
broom.mixed::tidyMCMC(b6.12)

b_gc == 0 # TRUE
```

This is known as Simposn's Paradox. Including `p` in this case can reverse the direction of an association between
G and C.

## 6.4 Confronting Confounding

Confounding, controlling for the wrong variable ruins inference

Education, wages, and unobserved. Unobserved affects both education and wages.

```{r}
dag_coords <-
  tibble(name = c("E", "U", "W"), x = c(1, 2, 3), y = c(1, 2, 1))

dagify(E ~ U, W ~ E + U, coords = dag_coords) %>%
  gg_simple_dag()
```

If we were able to design an experiment, where education levels are assigned at random,
that would isolate the causal path of E -> W.


```{r}
dagify(W ~ E + U, coords = dag_coords) %>%
  gg_simple_dag()
```

### Shutting the backdoor

Backdoor paths are paths where an arrow is entering X.

All of these, we are trying to measure Y ~ X, but the situation is not that simple

- The Fork: Y independent of X, Z -> X and Z -> Y, X does not infer anything about Y once Z is conditioned on
- The Pipe: X influences Z, Z influences Y. Z needs to be conditioned on to infer anything about X
- The Collider: Z is influenced by X and Y, X and Y are independent of each other.
- The Descendant

```{r}
d1 <-
  dagify(
    X ~ Z,
    Y ~ Z,
    coords = tibble(name = c("X", "Y", "Z"), x = c(1, 3, 2), y = c(2, 2, 1))
  )

d2 <-
  dagify(
    Z ~ X,
    Y ~ Z,
    coords = tibble(name = c("X", "Y", "Z"), x = c(1, 3, 2), y = c(2, 1, 1.5))
  )

d3 <-
  dagify(
    Z ~ X + Y,
    coords = tibble(name = c("X", "Y", "Z"), x = c(1, 3, 2), y = c(1, 1, 2))
  )

d4 <-
  dagify(
    Z ~ X + Y,
    D ~ Z,
    coords = tibble(
      name = c("X", "Y", "Z", "D"),
      x = c(1, 3, 2, 2),
      y = c(1, 1, 2, 1.05)
    )
  )

p1 <- gg_simple_dag(d1) + labs(subtitle = "The Fork")
p2 <- gg_simple_dag(d2) + labs(subtitle = "The Pipe")
p3 <- gg_simple_dag(d3) + labs(subtitle = "The Collider")
p4 <- gg_simple_dag(d4) + labs(subtitle = "The Descendant")

library(patchwork)

(p1 | p2 | p3 | p4) &
  theme(plot.subtitle = element_text(hjust = 0.5)) &
  plot_annotation(title = "The four elemental confounds")
```

#### Two Roads


```{r}
dag_coords <-
  tibble(
    name = c("A", "B", "C", "U", "X", "Y"),
    x = c(2, 2, 3, 1, 1, 3),
    y = c(4, 2, 3, 3, 1, 1)
  )

dagify(B ~ C + U, C ~ A, U ~ A, X ~ U, Y ~ C + X, coords = dag_coords) %>%
  gg_fancy_dag(x = 1, y = 3, circle = "U")
```


```{r}
dag_6.1 <-
  dagitty::dagitty(
    "dag {
    U [unobserved]
    X -> Y
    X <- U <- A -> C -> Y
    U -> B <- C
    }"
  )

dagitty::adjustmentSets(dag_6.1, exposure = "X", outcome = "Y")
```

For this dag, we would need to condition on either A or C. From an efficiency standpoint, C would be the preferred
variable to condition on if available.

The path of U -> B <- C is a collider, so that is already closed.

```{r}
dagitty::adjustmentSets(dag_6.1, exposure = "X", outcome = "Y", type = "all")
```

#### Backdoor Waffles


```{r}
dag_coords <-
  tibble(
    name = c("A", "D", "M", "S", "W"),
    x = c(1, 3, 2, 1, 3),
    y = c(1, 1, 2, 3, 3)
  )

dagify(A ~ S, D ~ A + M + W, M ~ A + S, W ~ S, coords = dag_coords) %>%
  gg_simple_dag()
```


```{r}
dag_6.2 <-
  dagitty::dagitty(
    "dag {
    A -> D
    A -> M -> D
    A <- S -> M
    S -> W -> D
    }"
  )

dagitty::adjustmentSets(dag_6.2, exposure = "W", outcome = "D")
```

Conditional independencies are when variables are independent of another once you
control for another variable. In this example, it is often looking at Fork situations.

```{r}
dagitty::impliedConditionalIndependencies(dag_6.2)
```


# Practice Problems

## Easy

#### 1. List three mechanisms by which multiple regression can produce false inferences about causal inference

- Multicolinearity
    - this produces the correct prediction output, but the coefficents are unstable because once you learn then the other offers little value.
- Post treatment or included variable bias
    - once we know that the item has the condition we are trying to prevent, knowing if there was treatment or not provides little to no value
- Collider bias
    - There is distorted associations based on the data generation process and how the sample is taken
    - There could be X and Y that are negatively correlated, but only because of their association with Z. X and Y are actually unrelated or have a different association across the sample

#### 2.Examples

- Multicolinearity: In football, there are a lot of correlated factors that are hard to tease out. Nothing as clear cut as left leg and right leg, but something like pass attempt volume could be driven by player/coach, it also is correlated with score differential. You pass more when you are down, and run more when you are up. Or even play slower. It would be uninformative to build a model where `Outcome ~ Pass_rate`, because the coefficient would likely be different once you control for score differential.


#### 3. Confound types
- The pipe: X -> Z -> Y
    - Y is conditional on Z, an including it blocks the path of X.
- The collider: X -> Z <- Y
    - Z is conditional on X and Y, conditioning on Z opens the path between X and Y - but neither X nor Y has causaul influence on the other
- The fork: X <- Z -> Y
    - Y is conditional on Z, and if you condition on Z it opens a correlation between X and Y, however then there is no correlation between X and Y
- The descendent: X -> Z; Y -> Z; Z -> D
    - D is conditonal on Z, Z is conditional on X and Y
    - There is a collider when looking at Z from X and Y, and a pipe when looking at D from X or Y
    - Conditioning on D will to a lesser extent condition on Z, partly opening the path from X to Y

## Medium

#### 1. Modify the dag

Original
```{r}
dag_coords <-
  tibble(
    name = c("A", "B", "C", "U", "X", "Y"),
    x = c(2, 2, 3, 1, 1, 3),
    y = c(4, 2, 3, 3, 1, 1)
  )

dagify(B ~ C + U, C ~ A, U ~ A, X ~ U, Y ~ C + X, coords = dag_coords) %>%
  gg_fancy_dag(x = 1, y = 3, circle = "U")
```


New  to include `V`, where it is an unobserved cause of `C` and `Y`. `C <- V -> Y`


```{r}
dag_coords <-
  tibble(
    name = c("A", "B", "C", "U", "X", "Y", "V"),
    x = c(2, 2, 3, 1, 1, 3, 3),
    y = c(4, 2, 3, 3, 1, 1, 2)
  )

dagify(B ~ C + U, C ~ A, U ~ A, X ~ U, Y ~ X, C ~ V, Y ~ V, coords = dag_coords) %>%
  gg_fancy_dag(x = 1, y = 3, circle = "U") |>
  gg_fancy_dag(x = 2, y = 2, circle = "V")
```

Now only U -> X -> Y can be measured.


```{r}

dag_m1 <-
  dagitty::dagitty(
    "dag {
    A -> C -> B
    A -> U -> B
    A -> U -> X -> Y
    C <- V -> Y
    }"
  )
dagitty::adjustmentSets(dag_m1, exposure = "X", outcome = "Y")
dagitty::impliedConditionalIndependencies(dag_m1)
```

In this new DAG, X and Y are downstream of only unknown factors, and we could measure the causal effect of Y ~ X directly (I assume with the caveat that the unknown factor could still be great, and this could just end up being a proxy?)

#### 2. Mulitcolinearity

Running a simulation with `X -> Z -> Y`, with `X` and `Z` highly correlated.

```{r}
n <- 100
d <-
  tibble(
    X = rnorm(n, mean = 0, sd = 1),
    Z = rnorm(n, mean = X),
    Y = rnorm(n, Z)
  )
```


```{r}
p2 <-
  brm(
    data = d,
    family = gaussian,
    Y ~ 1 + X + Z,
    prior = c(
      prior(normal(0, 3), class = Intercept),
      prior(normal(0, 3), class = b),
      prior(exponential(1), class = sigma)
    ),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    cores = 4,
    seed = 6,
    backend= "cmdstanr"
  )
```


```{r}
print(p2)
mcmc_plot(p2)
```


```{r}
dag_m2 <-
  dagitty::dagitty(
    "dag {
    X -> Z -> Y
    }"
  )
dagitty::adjustmentSets(dag_m2, exposure = "X", outcome = "Y")
dagitty::impliedConditionalIndependencies(dag_m2)
```

The model is different than when we did the simulation of the legs and height. In the height model, legs were highly correlated, but it could not determine which leg contained more signal. Once you knew one leg, knowing the other made little difference. This resulted in the coefficients having wide error bands above and around zero.

In this model, because Z is conditional on X and Y is conditional on Z, it was able to isolate that the coefficient of Z should be larger, and once you knew Z, X was did not provide a lot of value.

X is conditional on Y given Z. Z is a mediator. The main takeaway is that if you run a model on `Y ~ X`, `X` will have a positive coefficient (in the simulated data this is around 1). However, if you condition `Z` with `Y ~ X + Z`, the coefficiennt drops for `X` to around -0.1, with the `Z` coefficient being around 0.8. If you tried to do causal inference without `Z`, you would get misleading results.

#### 3. DAGs

The first one I thought would be `A`, however the results are that it is `Z`, and `A` is independent on `X` once you condition on `Z`.

```{r}
dag_m3.1 <-
  dagitty::dagitty(
    "dag {
    A -> Z -> X -> Y
    A -> Z -> Y
    A -> Y
    Z -> Y
    X -> Y
    }"
  )
dagitty::adjustmentSets(dag_m3.1, exposure = "X", outcome = "Y")
dagitty::impliedConditionalIndependencies(dag_m3.1)

# { Z }
# A _||_ X | Z
```




##### 2. Upper right

I think that this one will need to condition on Z or A? There are no backdoor paths to X

```{r}
dag_m3.2 <-
  dagitty::dagitty(
    "dag {
    A -> Z -> Y
    A -> Y
    X -> Z -> Y
    X -> Y
    Z -> Y
    }"
  )
dagitty::adjustmentSets(dag_m3.2, exposure = "X", outcome = "Y")
dagitty::impliedConditionalIndependencies(dag_m3.2)

#  {}
# A _||_ X

```

Hmm, so it said that there were no adjustmet sets, and that A is independent of X. That makes sense, it just feels like you would need to condition on something with the complexity of the graph. I guess that isn't necessarily how it works. You want to get an understanding, but you will never be able to control for everything. You just want to make sure the association you are reporting isn't spurious.

##### 3. Lower left

I think this one will need to condition on `A`, as `X` is a pipe for `A -> X -> Y`.

```{r}
dag_m3.3 <-
  dagitty::dagitty(
    "dag {
    A -> Z
    A -> X
    X -> Y
    X -> Z
    Y -> Z
    }"
  )
dagitty::adjustmentSets(dag_m3.3, exposure = "X", outcome = "Y")
dagitty::impliedConditionalIndependencies(dag_m3.3)

# {}
# A _||_ Y | X
```

Hmm, this one says that there were no adjustment sets, and A is independent of Y given X. Okay, and that I guess makes sense too back to the multicolinearity example. We want to make sure we are measuring `Y ~ X` and not `Y ~ A`.

##### 4. Lower right

I think this one will be `Z`

```{r}
dag_m3.4 <-
  dagitty::dagitty(
    "dag {
    A -> X
    A -> Z
    X -> Z
    Z -> Y
    X -> Y
    }"
  )
dagitty::adjustmentSets(dag_m3.4, exposure = "X", outcome = "Y")
dagitty::impliedConditionalIndependencies(dag_m3.4)
```

Ah, damn this one is `A`. `A` blcoks the backdoor path to `X` and is a common cause of `X` and `Z`. `Z` could be a post-treatment effect, and wouldn't be included.

### Hard

```{r}
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce

rm(WaffleDivorce)
detach(package:rethinking, unload = T)

```

#### 1.




```{r}
d <- d |>
  as_tibble() |>
  mutate(
    D_std = rethinking::standardize(Divorce),
    W_std = rethinking::standardize(WaffleHouses)
  )
```

```{r}
p3.1 <- brm(
  D_std ~ 1 + W_std + South,
  data = d,
  family = gaussian(),
  prior = c(
    prior(normal(10, 10), class = Intercept),
    prior(normal(0, 1), class = b),
    prior(exponential(1), class = sigma)
  ),
  iter = 2000,
  warmup = 500,
  chains = 4,
  cores = 4,
  seed = 5,
  backend = 'cmdstanr'
)
```


```{r}
print(p3.1)
mcmc_plot(p3.1)


as_draws_df(p3.1) |>
  select(-lprior, -lp__) |>
  pivot_longer(everything()) |>
  group_by(name) |>
  median_hdci()


as_draws_df(p3.1) %>%
  ggplot(aes(x = b_W_std, y = 0)) +
  tidybayes::stat_halfeye(
    point_interval = median_qi,
    fill = "steelblue",
    .width = .95
  )
```

Once we account for `South` in the WaffleHouse model, the coefficient of Waffle houses is close to 0, with a large standard error ranging from above and below zero. Essentially, it has little effect.

#### Foxes


```{r}
library(rethinking)
data(foxes)
d <- foxes

rm(foxes)
detach(package:rethinking, unload = T)
```

#### 3.`weight ~ area`

Area -> avgfood -> weight
Area -> avgfood -> groupsize -> weight

```{r}
dag_h3 <-
  dagitty::dagitty(
    "dag {
    Area -> avgfood -> weight
    Area -> avgfood -> groupsize -> weight
    }"
  )
dagitty::adjustmentSets(dag_h3, exposure = "Area", outcome = "weight")
dagitty::impliedConditionalIndependencies(dag_h3)
```

```{r}
d <- d |>
  as_tibble() |>
  mutate(
    avg_food_std = rethinking::standardize(avgfood),
    group_size_std = rethinking::standardize(groupsize),
    area_std = rethinking::standardize(area),
    weight_std = rethinking::standardize(weight)
  )
```


```{r}
p3.3_prior <- brm(
  weight_std ~ 1 + area_std,
  data = d,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.25), class = Intercept),
    prior(normal(0, 0.25), class = b),
    prior(exponential(1), class = sigma)
  ),
  iter = 2000,
  warmup = 500,
  chains = 4,
  cores = 4,
  seed = 5,
  backend = 'cmdstanr',
  sample_prior = 'only'
)

# prior predictive checks! values are within range
d |>
  add_epred_draws(p3.3_prior) |>
  ungroup() |>
  select(weight_std, .epred) |>
  pivot_longer(everything()) |>
  summarise(
    across(value, list(mean = mean, min = min, max = max)),
    .by = name
  )

plot(p3.3_prior)
pp_check(p3.3_prior, ndraws = 100)



```


```{r}
p3.3 <- brm(
  weight_std ~ 1 + area_std,
  data = d,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.25), class = Intercept),
    prior(normal(0, 0.25), class = b),
    prior(exponential(1), class = sigma)
  ),
  iter = 2000,
  warmup = 500,
  chains = 4,
  cores = 4,
  seed = 5,
  backend = 'cmdstanr',
)
```


```{r}
pp_check(p3.3, ndraws = 50)
mcmc_plot(p3.3)

as_draws_df(p3.3) |>
  summary()

summary(p3.3)
```

The output shows that the coefficient of area is around 0, with the upper and lower bounds being equal above and below zero. It's not clear that changing area has any affect on weight (overall health).


#### 4. `weight ~ avgfood + area`

Theres an conditional independence, where area is independent of weight when conditioned on avgfood


```{r}
dagitty::adjustmentSets(dag_h3, exposure = "avgfood", outcome = "weight")
dagitty::impliedConditionalIndependencies(dag_h3)
```

```{r}
p3.4_prior <- brm(
  weight_std ~ 1 + area_std + avg_food_std,
  data = d,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.25), class = Intercept),
    prior(normal(0, 0.25), class = b),
    prior(exponential(1), class = sigma)
  ),
  iter = 1000,
  warmup = 500,
  chains = 4,
  cores = 4,
  seed = 5,
  backend = 'cmdstanr',
  sample_prior = 'only'
)

# prior predictive checks! values are within range
d |>
  add_epred_draws(p3.4_prior) |>
  ungroup() |>
  select(weight_std, .epred) |>
  pivot_longer(everything()) |>
  summarise(
    across(value, list(mean = mean, min = min, max = max)),
    .by = name
  )
```


```{r}
p3.4 <- brm(
  weight_std ~ 1 + avg_food_std ,
  data = d,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.25), class = Intercept),
    prior(normal(0, 0.25), class = b),
    prior(exponential(1), class = sigma)
  ),
  iter = 2000,
  warmup = 500,
  chains = 4,
  cores = 4,
  seed = 5,
  backend = 'cmdstanr'
)
```


```{r}
pp_check(p3.4, ndraws = 50)
mcmc_plot(p3.4)

as_draws_df(p3.4) |>
  summary()

summary(p3.4)
```

Area is independent of weight, when conditioned on avgfood. Using avgfood in the model, it shows having a slightly negative association with weight.

There are no other variables needed to model weight ~ avgfood given the DAG. Knowing that it would be unobserved variables going into eagh avgfood and weight

#### 5. `weight ~ groupsize`


```{r}
dagitty::adjustmentSets(dag_h3, exposure = "groupsize", outcome = "weight")
```

For groupsize, we would need to add avgfood along with it into the model. This is similar to the family example, where avgfood affects groupsize and weight, similar to grandparent affect parent and child. groupsize affects weight, similar to parent affecting child.

This is a descendant confounder.

```{r}
p3.5 <- brm(
  weight_std ~ 1 + group_size_std + avg_food_std,
  data = d,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.25), class = Intercept),
    prior(normal(0, 0.25), class = b),
    prior(exponential(1), class = sigma)
  ),
  iter = 2000,
  warmup = 500,
  chains = 4,
  cores = 4,
  seed = 5,
  backend = 'cmdstanr'
)
```


```{r}
pp_check(p3.5, ndraws = 50)
mcmc_plot(p3.5)

as_draws_df(p3.5) |>
  summary()

summary(p3.4)
```

Wow! group size has a large affect on weight negatively, and avgfood has a strong positive association.

Not sure what he is asking for. A logical explanation is that once you account for the size of the group, then avg amount of food available fits the intuition that more food leads to more weight.

When modeling food availability alone, leaving out group size removes key information needed to understand why there would be a negative association with more food and less weight.

#### 6. My own problem

Let's see, we want to model passes in a game.

coach -> passes
qb -> passes
coach -> qb -> passes
opponent -> passes
opponent -> turnovers -> passes



```{r}
my_dag <-
  dagitty::dagitty(
    "dag {
    coach -> passes
    qb -> passes
    coach -> qb -> passes
    opponent -> passes
    opponent -> turnovers -> passes
    }"
  )
dagitty::adjustmentSets(my_dag, exposure = "qb", outcome = "passes")
dagitty::impliedConditionalIndependencies(my_dag)
```

These are all independent things. To model the causal affect of QB on pass volume, you would need to include coach.

```{r}

```
